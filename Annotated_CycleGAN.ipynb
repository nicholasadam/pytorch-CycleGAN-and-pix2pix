{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9be3d715-de60-475f-809c-1494e4643109",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Annotated CycleGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3a9e01-1da5-4227-a49b-991623e346bc",
   "metadata": {},
   "source": [
    "As we are building a web app \"deepfake\" which is of image translation based on GAN model, I present an \"annotated\" version of the pytorch-based-CycleGAN model in the form of a line-by-line implementation. It will cover data preparation, different GAN models, training schemes, optimization tricks for both training and computation. After completing this notebook, you will know:\n",
    "\n",
    "- How to implement the discriminator and generator models.\n",
    "- How to define composite models to train the two aforementioned models via adversarial and cycle loss.\n",
    "- How to do the model inference to generate the translated image.\n",
    "\n",
    "To follow along you will first need to install the libraries listed in `requirements-an.txt`. The workflow should be considered as a starting point of image translation tasks with the enrichment of GAN model.\n",
    "\n",
    "> My comments are blockquoted. The main text is based on the CycleGAN/Pytorch documentation.\n",
    "> Wangsu Hu (wangsu.hu1234@gmail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e67c89f-4428-4006-8bab-944ef3f6c0a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prelims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cda74abd-2177-466e-a899-a301bfeaea76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/Users/wangsuhu/Google Drive/Github/pytorch-CycleGAN-and-pix2pix/env/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -r requirements-an.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dbe0fdf-2663-43b3-9a7f-aa53552d2e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import functools\n",
    "from torch.nn import init\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "import time\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4c38ec-cc00-45e8-802c-711963d09165",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Background\n",
    "\n",
    "The Cycle Generative Adversarial Network, or CycleGAN, is an approach to training a deep convolutional neural network for image-to-image translation tasks.\n",
    "\n",
    "Unlike other GAN models for image translation, the CycleGAN does not require a dataset of paired images. \n",
    "\n",
    "The CycleGAN model was described by Jun-Yan Zhu, et al. in their 2017 paper titled [“Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks.”](https://arxiv.org/abs/1703.10593)\n",
    "\n",
    "Here is the official github repo released and maintained by the author list [pytorch-CycleGAN-and-pix2pix](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix). \n",
    "\n",
    "The model architecture is comprised of two generator models: \n",
    "- one generator (Generator G_A2B) for generating images for the second class (class-B) based on the input image for the first class (class-A)\n",
    "- another generator (Generator G_B2A) for generating images for the first class (class-A) based on the input image for the second class (class-B)\n",
    "\n",
    "> as you can see, unlike normal GAN using randomness, the generator of CycleGAN is conditional on the input image and the input image can be a real image or a generated image. See the generator blocks in workflow below.\n",
    "\n",
    "    - real image for class-A -> G_A2B -> generated image for class-B \n",
    "\n",
    "    - generated image for class-A -> G_A2B -> generated image for class-B \n",
    "\n",
    "    - real image for class-B -> G_B2A -> generated image for class-A \n",
    "\n",
    "    - generated image for class-B -> G_B2A -> generated image for class-A \n",
    "\n",
    "    - real image for class-A -> G_B2A -> generated image for class-A \n",
    "\n",
    "    - real image for class-B -> G_A2B -> generated image for class-B \n",
    "\n",
    "> the last two seem tricky, it corresponds to identity mapping in CycleGAN which will be explained later.\n",
    "\n",
    "- one discriminator (Discriminator D_A) takes images from class-A then predicts whether they are real or fake.\n",
    "- one discriminator (Discriminator D_B) takes images from class-B then predicts whether they are real or fake.\n",
    "\n",
    "> similar to generators, the input image of discriminator can be a real image or a generated image.\n",
    "\n",
    "    - real image for class-A -> D_A -> real/fake\n",
    "\n",
    "    - generated image for class-A -> D_A -> real/fake\n",
    "\n",
    "    - real image for class-B -> D_B -> real/fake\n",
    "\n",
    "    - generated image for class-B -> D_B -> real/fake\n",
    "\n",
    "Why does such GAN model called \"Cycle\"? \n",
    "\n",
    "The discriminator and generator models are trained in an adversarial zero-sum process, like normal GAN models. The generators learn to better fool the discriminators and the discriminator learn to better detect fake images. Together, the models find an equilibrium during the training process.\n",
    "\n",
    "Additionally, the generator models are regularized to not just create new images in the target domain, but instead translate more reconstructed versions of the input images from the source domain. This is achieved by using generated images as input to the corresponding generator model and comparing the output image to the original images. Passing an image through both generators is called a cycle. Together, each pair of generator models are trained to better reproduce the original source image, referred to as cycle consistency.\n",
    "\n",
    "    - real image for class-A -> G_A2B -> generated image for class-B -> G_B2A -> generated image for class-A vs the input real image for class-A\n",
    "\n",
    "    - real image for class-B -> G_B2A -> generated image for class-A -> G_A2B -> generated image for class-B vs the input real image for class-B\n",
    "\n",
    "Finally, there is identity mapping when a generator model is expected to output an image without translation when provided an example from the target domain. \n",
    "\n",
    "    - real image for class-A -> G_B2A -> generated image for class-A\n",
    "\n",
    "    - real image for class-B -> G_A2B -> generated image for class-B\n",
    "\n",
    "\n",
    "see the following flow chart for your reference.\n",
    "\n",
    "![work_flow](workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721eeb7e-118a-4a92-b58e-4a9757543e85",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473ceeb7-7125-4937-a5d3-94220bf83f4c",
   "metadata": {},
   "source": [
    "We prepare the \"horses2zebra\" dataset which is a subset imagenet dataset and contains around 111 megabytes and can be downloaded from the CycleGAN webpage:\n",
    "\n",
    "[Download](https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip)\n",
    "> after download, put the folder under `annotated_cyclegan/data/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ee1875-e459-4879-8d09-f10dfda1295a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### let's first look at what data look like, we randomly selected one image from each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0527c971-248f-4c45-b01a-cd93a97e6e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "horse_sample = \"annotated_cyclegan/data/horse2zebra/testA/n02381460_1000.jpg\"\n",
    "img = plt.imread(horse_sample)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2d457d-9076-47f3-b791-cea43fcc9e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "zebra_sample = \"annotated_cyclegan/data/horse2zebra/testB/n02391049_100.jpg\"\n",
    "img = plt.imread(zebra_sample)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f227fa87-97d1-4e50-b18c-cd662c8072d0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Design Dataset Class\n",
    "\n",
    "Code for propressing data samples can get messy and hard to maintain; we ideally want our dataset code to be decoupled from our model training code for better readability and modularity. Pytorch provides two data primitives: `torch.utils.data.DataLoader` and `torch.utils.data.Dataset` that allow use to use pre-loaded datasets as well as our own data. `Dataset` stores the samples and their corresponding labels; and `DataLoader` wraps an iterable around the the Dataset to enable easy access to the samples. \n",
    "> the raw CycleGAN code did the similar thing but in a more customized way. More details can be found under `data/unaligned_dataset.py` and `data/base_dataset.py` and `data/__init__.py`.\n",
    "\n",
    "As our dataset is not so large (110 MB), we can save them as arrays/vectors then feed into memory later. The normal opt for image pre-processing contains transformation steps such as colorization, scale, crop, and etc. Here we applied what the origin cyclegan paper does, i.e., creating a `UnalignedDataset` which include: 1. load raw image; 2. convert it into tensor; 3. normalize tensor along each color channel; 4. wrap them into a list object. \n",
    "> the raw class was defined in `data/unaligned_dataset.py`, here we simplify the code for our own CycleGAN-only experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1091ffa1-ef72-4513-8fdf-fa6bf3de0eb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_transform():\n",
    "    \"\"\"\n",
    "    function to do the transformation steps: image -> tensor and normalization.\n",
    "    \"\"\"\n",
    "    transform_list = []\n",
    "    transform_list += [transforms.ToTensor()]\n",
    "    transform_list += [transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))] #assume we are dealing with jpg image which contains 3 color channels.\n",
    "    return transforms.Compose(transform_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc4f9bd4-b11e-4613-9cc4-8465ea9fec99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UnalignedDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    This dataset class can load unaligned/unpaired datasets.\n",
    "\n",
    "    It requires two directories to host training images from domain A '/path/to/data/trainA'\n",
    "    and from domain B '/path/to/data/trainB' respectively.\n",
    "    You can train the model with the dataset flag '--dataroot /path/to/data'.\n",
    "    Similarly, you need to prepare two directories:\n",
    "    '/path/to/data/testA' and '/path/to/data/testB' during test time.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, source_class_A_folder, source_class_B_folder):\n",
    "        \"\"\"Initialize this dataset class.\n",
    "\n",
    "        Parameters:\n",
    "            opt (Option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions\n",
    "        \"\"\"\n",
    "        self.dir_A = source_class_A_folder  # create a path '/path/to/data/trainA'\n",
    "        self.dir_B = source_class_B_folder  # create a path '/path/to/data/trainB'\n",
    "        self.transformation = get_transform()\n",
    "        self.A_images = self.__transform_images__(self.dir_A)\n",
    "        self.B_images = self.__transform_images__(self.dir_B)\n",
    "        self.A_size, self.B_size = len(self.A_images), len(self.B_images)\n",
    "        \n",
    "        \n",
    "    def __transform_images__(self, image_folder):\n",
    "        \"\"\"Return a transformed images list.\n",
    "\n",
    "        Parameters:\n",
    "            image_folder      -- folder contains your images assuming to be of `Image` readable format (such as JPEG, JPG, PNG)\n",
    "\n",
    "        Returns a list that image tensor\n",
    "        \"\"\"\n",
    "        images = []\n",
    "        for root, _, fnames in sorted(os.walk(image_folder)):\n",
    "            for fname in fnames:\n",
    "                path = os.path.join(root, fname)\n",
    "                image = Image.open(path).convert('RGB')\n",
    "                images += [self.transformation(image)]\n",
    "        return images\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return a data point and its metadata information.\n",
    "\n",
    "        Parameters:\n",
    "            index (int)      -- a random integer for data indexing\n",
    "\n",
    "        Returns a dictionary that contains A, B\n",
    "            A (tensor)       -- an image in the input domain\n",
    "            B (tensor)       -- its corresponding image in the target domain\n",
    "        \"\"\"\n",
    "\n",
    "        return {'A': self.A_images[index % self.A_size], 'B': self.B_images[index % self.B_size]}\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of images in the dataset.\n",
    "\n",
    "        As we have two datasets with potentially different number of images,\n",
    "        we take a maximum of\n",
    "        \"\"\"\n",
    "        return max(self.A_size, self.B_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e67ba8f8-9abf-4285-8387-4e194a872e15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = UnalignedDataset(\"annotated_cyclegan/data/horse2zebra/trainA/\", \"annotated_cyclegan/data/horse2zebra/trainB/\")\n",
    "test_dataset = UnalignedDataset(\"annotated_cyclegan/data/horse2zebra/testA/\", \"annotated_cyclegan/data/horse2zebra/testB/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49477ed-1547-48f5-a841-3a763744d0db",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Design DataLoader Class\n",
    "\n",
    "Lets create a basic DataLoader pytorch class.\n",
    "> CycleGAN do such thing inside the CustomDatasetDataLoader class in `data/__init__.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e6ccc39-36a5-4861-a604-0e446854f13a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def DataLoader(train_dataset, test_dataset, train_batch, test_batch):\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = train_batch, shuffle = True)\n",
    "    test_loader = torch.utils.data.DataLoader(train_dataset, batch_size = test_batch, shuffle = False)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd99aef2-9462-4dc2-b3c8-e39b4c84b2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = DataLoader(train_dataset, test_dataset, train_batch = 4, test_batch = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64b7bb1-e65e-4e02-8f8e-cab75aa4783e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Design Model\n",
    "\n",
    "As mentioned in section \"Background\", The architecture is comprised of four models, two discriminator models, and two generator models.\n",
    "\n",
    "The discriminator is a deep convolutional neural network that performs image classification. It takes a source image as input and predicts the likelihood of whether the target image is a real or fake image. Two discriminator models are used, one for class-A (horses) and one for class-B (zebras).\n",
    "\n",
    "The discriminator design is based on the effective receptive field of the model, which defines the relationship between one output of the model to the number of pixels in the input image. This is called a PatchGAN model and is carefully designed so that each output prediction of the model maps to a 70×70 square or patch of the input image. The benefit of this approach is that the same model can be applied to input images of different sizes, e.g. larger or smaller than 256×256 pixels.\n",
    "\n",
    "The output of the model depends on the size of the input image but may be one value or a square activation map of values. Each value is a probability for the likelihood that a patch in the input image is real. These values can be averaged to give an overall likelihood or classification score if needed.\n",
    "\n",
    "A pattern of Convolutional-BatchNorm-LeakyReLU layers is used in the model, which is common to deep convolutional discriminator models. Unlike other models, the CycleGAN discriminator uses InstanceNormalization instead of BatchNormalization. It is a very simple type of normalization and involves standardizing (e.g. scaling to a standard Gaussian) the values on each output feature map, rather than across features in a batch.\n",
    "\n",
    "An implementation of instance normalization is provided in the keras-contrib project that provides early access to community supplied Keras features.\n",
    "\n",
    "> we will use the code from `models/cycle_gan_model.py` and `models/networks.py`.\n",
    "\n",
    "> the hyperparameter setting is following the default value stored in `options/BaseOptions.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8899296-dbec-4e52-ac7d-77f907c5bfb7",
   "metadata": {},
   "source": [
    "### Design discriminator \n",
    "\n",
    "Here we assume some hyperparamters as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d99eb4fe-2511-4f99-911e-f75524912419",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_nc = 3 # -- the number of channels in input images\n",
    "ndf = 64 # -- the number of filters in the first conv layer\n",
    "n_layers_D = 3 # -- the number of conv layers in the discriminator; effective when netD=='n_layers'\n",
    "norm = \"instance\" # -- the type of normalization layers used in the network.\n",
    "init_type = \"normal\" # -- the name of the initialization method.\n",
    "init_gain = 0.02 # -- scaling factor for normal, xavier and orthogonal.\n",
    "netD = \"basic\" # -- default PatchGAN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9e0eddb-b0a0-4085-ad6f-3904952c5ab2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NLayerDiscriminator(nn.Module):\n",
    "    \"\"\"Defines a PatchGAN discriminator\"\"\"\n",
    "\n",
    "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d):\n",
    "        \"\"\"Construct a PatchGAN discriminator\n",
    "\n",
    "        Parameters:\n",
    "            input_nc (int)  -- the number of channels in input images\n",
    "            ndf (int)       -- the number of filters in the last conv layer\n",
    "            n_layers (int)  -- the number of conv layers in the discriminator\n",
    "            norm_layer      -- normalization layer\n",
    "        \"\"\"\n",
    "        super(NLayerDiscriminator, self).__init__()\n",
    "        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "\n",
    "        kw = 4\n",
    "        padw = 1\n",
    "        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]\n",
    "        nf_mult = 1\n",
    "        nf_mult_prev = 1\n",
    "        for n in range(1, n_layers):  # gradually increase the number of filters\n",
    "            nf_mult_prev = nf_mult\n",
    "            nf_mult = min(2 ** n, 8)\n",
    "            sequence += [\n",
    "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n",
    "                norm_layer(ndf * nf_mult),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            ]\n",
    "\n",
    "        nf_mult_prev = nf_mult\n",
    "        nf_mult = min(2 ** n_layers, 8)\n",
    "        sequence += [\n",
    "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n",
    "            norm_layer(ndf * nf_mult),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "\n",
    "        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]  # output 1 channel prediction map\n",
    "        self.model = nn.Sequential(*sequence)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Standard forward.\"\"\"\n",
    "        return self.model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cd5e9ba-fcef-4ad6-a64e-23d4cbd4c842",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_weights(net):\n",
    "    \"\"\"Initialize network weights.\n",
    "\n",
    "    Parameters:\n",
    "        net (network)   -- network to be initialized\n",
    "        init_type (str) -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n",
    "        init_gain (float)    -- scaling factor for normal, xavier and orthogonal.\n",
    "\n",
    "    We use 'normal' in the original pix2pix and CycleGAN paper. But xavier and kaiming might\n",
    "    work better for some applications. Feel free to try yourself.\n",
    "    \"\"\"\n",
    "    def init_func(m):  # define the initialization function\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "            init.normal_(m.weight.data, 0.0, init_gain)\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                init.constant_(m.bias.data, 0.0)\n",
    "        elif classname.find('BatchNorm2d') != -1:  # BatchNorm Layer's weight is not a matrix; only normal distribution applies.\n",
    "            init.normal_(m.weight.data, 1.0, init_gain)\n",
    "            init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    print('initialize network with %s' % init_type)\n",
    "    net.apply(init_func)  # apply the initialization function <init_func>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f39f977-122a-4c12-923a-4677c0b6d842",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def define_D():\n",
    "    \"\"\"Create a discriminator \n",
    "\n",
    "    Returns a discriminator\n",
    "\n",
    "    Our current implementation provides three types of discriminators:\n",
    "        [basic]: 'PatchGAN' classifier described in the original pix2pix paper.\n",
    "        It can classify whether 70×70 overlapping patches are real or fake.\n",
    "        Such a patch-level discriminator architecture has fewer parameters\n",
    "        than a full-image discriminator and can work on arbitrarily-sized images\n",
    "        in a fully convolutional fashion.\n",
    "\n",
    "        [n_layers]: With this mode, you can specify the number of conv layers in the discriminator\n",
    "        with the parameter <n_layers_D> (default=3 as used in [basic] (PatchGAN).)\n",
    "\n",
    "        [pixel]: 1x1 PixelGAN discriminator can classify whether a pixel is real or not.\n",
    "        It encourages greater color diversity but has no effect on spatial statistics.\n",
    "\n",
    "    The discriminator has been initialized by <init_net>. It uses Leakly RELU for non-linearity.\n",
    "    \"\"\"\n",
    "    net = None\n",
    "    norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False) # as we set norm = \"instance\"\n",
    "    net = NLayerDiscriminator(input_nc, ndf, n_layers=3, norm_layer=norm_layer) # as we set netD = \"basic\"\n",
    "    \n",
    "    init_weights(net)\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5ba38f8-2814-44b0-9e44-05b299901085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize network with normal\n",
      "initialize network with normal\n"
     ]
    }
   ],
   "source": [
    "D_A = define_D()\n",
    "D_B = define_D()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58549e9e-64d2-43e9-8d2e-b68a4798fff4",
   "metadata": {},
   "source": [
    "### Design generator \n",
    "\n",
    "Here we assume some hyperparamters as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "923c33d4-5c2c-4cb5-b5cb-7d5a3fabb57a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_nc = 3 # -- the number of channels in input images\n",
    "output_nc = 3 # -- the number of channels in output images\n",
    "ngf = 64 # -- the number of filters in the first conv layer\n",
    "n_layers_D = 3 # -- the number of conv layers in the discriminator; effective when netD=='n_layers'\n",
    "norm = \"instance\" # -- the type of normalization layers used in the network.\n",
    "init_type = \"normal\" # -- the name of the initialization method.\n",
    "init_gain = 0.02 # -- scaling factor for normal, xavier and orthogonal.\n",
    "netG = \"resnet_9blocks\" # -- default CycleGAN generator's architecture\n",
    "padding_type = 'reflect' # -- the name of padding layer in conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8f02240-ff58-4a6c-96f2-58ec7cbaf88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(nn.Module):\n",
    "    \"\"\"Define a Resnet block\"\"\"\n",
    "\n",
    "    def __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
    "        \"\"\"Initialize the Resnet block\n",
    "\n",
    "        A resnet block is a conv block with skip connections\n",
    "        We construct a conv block with build_conv_block function,\n",
    "        and implement skip connections in <forward> function.\n",
    "        Original Resnet paper: https://arxiv.org/pdf/1512.03385.pdf\n",
    "        \"\"\"\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n",
    "\n",
    "    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
    "        \"\"\"Construct a convolutional block.\n",
    "\n",
    "        Parameters:\n",
    "            dim (int)           -- the number of channels in the conv layer.\n",
    "            padding_type (str)  -- the name of padding layer: reflect | replicate | zero\n",
    "            norm_layer          -- normalization layer\n",
    "            use_dropout (bool)  -- if use dropout layers.\n",
    "            use_bias (bool)     -- if the conv layer uses bias or not\n",
    "\n",
    "        Returns a conv block (with a conv layer, a normalization layer, and a non-linearity layer (ReLU))\n",
    "        \"\"\"\n",
    "        conv_block = []\n",
    "        p = 0\n",
    "        conv_block += [nn.ReflectionPad2d(1)] # as we set padding = \"reflect\"\n",
    "\n",
    "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim), nn.ReLU(True)]\n",
    "        if use_dropout:\n",
    "            conv_block += [nn.Dropout(0.5)]\n",
    "\n",
    "        p = 0\n",
    "        conv_block += [nn.ReflectionPad2d(1)] # as we set padding = \"reflect\"\n",
    "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim)]\n",
    "\n",
    "        return nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function (with skip connections)\"\"\"\n",
    "        out = x + self.conv_block(x)  # add skip connections\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d89f6be-65bd-46af-9811-653dd471f28c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResnetGenerator(nn.Module):\n",
    "    \"\"\"Resnet-based generator that consists of Resnet blocks between a few downsampling/upsampling operations.\n",
    "\n",
    "    We adapt Torch code and idea from Justin Johnson's neural style transfer project(https://github.com/jcjohnson/fast-neural-style)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, norm_layer, use_dropout, n_blocks):\n",
    "        \"\"\"Construct a Resnet-based generator\n",
    "\n",
    "        Parameters:\n",
    "            norm_layer          -- normalization layer\n",
    "            use_dropout (bool)  -- if use dropout layers\n",
    "            n_blocks (int)      -- the number of ResNet blocks\n",
    "        \"\"\"\n",
    "        super(ResnetGenerator, self).__init__()\n",
    "        use_bias = norm_layer.func == nn.InstanceNorm2d # as we set norm = \"instance\" using functools.partial\n",
    "\n",
    "        model = [nn.ReflectionPad2d(3),\n",
    "                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=use_bias),\n",
    "                 norm_layer(ngf),\n",
    "                 nn.ReLU(True)]\n",
    "\n",
    "        n_downsampling = 2\n",
    "        for i in range(n_downsampling):  # add downsampling layers\n",
    "            mult = 2 ** i\n",
    "            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1, bias=use_bias),\n",
    "                      norm_layer(ngf * mult * 2),\n",
    "                      nn.ReLU(True)]\n",
    "\n",
    "        mult = 2 ** n_downsampling\n",
    "        for i in range(n_blocks):       # add ResNet blocks\n",
    "\n",
    "            model += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)]\n",
    "\n",
    "        for i in range(n_downsampling):  # add upsampling layers\n",
    "            mult = 2 ** (n_downsampling - i)\n",
    "            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n",
    "                                         kernel_size=3, stride=2,\n",
    "                                         padding=1, output_padding=1,\n",
    "                                         bias=use_bias),\n",
    "                      norm_layer(int(ngf * mult / 2)),\n",
    "                      nn.ReLU(True)]\n",
    "        model += [nn.ReflectionPad2d(3)]\n",
    "        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n",
    "        model += [nn.Tanh()]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Standard forward\"\"\"\n",
    "        return self.model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f4f0e69-c2a0-486e-b5dc-2570a6497b7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def define_G(use_dropout=True): # for training we will set dropout = True\n",
    "    \"\"\"Create a generator\n",
    "\n",
    "    Parameters:\n",
    "        use_dropout (bool) -- if use dropout layers.\n",
    "\n",
    "    Returns a generator\n",
    "\n",
    "    Our current implementation provides two types of generators:\n",
    "        U-Net: [unet_128] (for 128x128 input images) and [unet_256] (for 256x256 input images)\n",
    "        The original U-Net paper: https://arxiv.org/abs/1505.04597\n",
    "\n",
    "        Resnet-based generator: [resnet_6blocks] (with 6 Resnet blocks) and [resnet_9blocks] (with 9 Resnet blocks)\n",
    "        Resnet-based generator consists of several Resnet blocks between a few downsampling/upsampling operations.\n",
    "        We adapt Torch code from Justin Johnson's neural style transfer project (https://github.com/jcjohnson/fast-neural-style).\n",
    "\n",
    "    The generator has been initialized by <init_net>. It uses RELU for non-linearity.\n",
    "    \"\"\"\n",
    "    net = None\n",
    "    norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False) # as we set norm = \"instance\"\n",
    "    net = ResnetGenerator(norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=9) # as we set netG = \"resnet_9blocks\"\n",
    "    init_weights(net)\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "090db72f-8310-4c1e-a546-84eaaffa5159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize network with normal\n",
      "initialize network with normal\n"
     ]
    }
   ],
   "source": [
    "G_A2B = define_G()\n",
    "G_B2A = define_G()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac4b79e-913a-4abb-a1f7-6bc95b0b8b1e",
   "metadata": {},
   "source": [
    "### Design CycleGAN model\n",
    "\n",
    "we first create the basic `nn.Module` torch class of CycleGAN with `__init__()` and `forward()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57b60bcb-42c4-4df2-997b-0133eef50753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CycleGANModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CycleGANModel, self).__init__()\n",
    "        self.netG_A2B = G_A2B\n",
    "        self.netG_B2A = G_B2A\n",
    "        self.netD_A = D_A\n",
    "        self.netD_B = D_B\n",
    "        \n",
    "    def forward(self, real_A, real_B): # recall the workflow\n",
    "        self.fake_B = self.netG_A2B(real_A)  # G_A2B(A) # -- generated translated image for class B based on the input real image for class A\n",
    "        self.pred_fake_B = self.netD_B(self.fake_B) #D_B(G_A2B(A)) -- predict real/fake of the generated image for class B based on the input real image for class A\n",
    "        self.rec_A = self.netG_B2A(self.fake_B)   # G_A2B(A) # -- generated translated image for class A based on the input generated image for class B\n",
    "        \n",
    "        self.fake_A = self.netG_B2A(real_B)  # G_A2B(A) # -- generated translated image for class A based on the input real image for class B\n",
    "        self.pred_fake_A = self.netD_A(self.fake_A) #D_A(G_B2A(B)) -- predict real/fake of the generated image for class A based on the input real image for class B\n",
    "        self.rec_B = self.netG_A2B(self.fake_A)   # G_A2B(A) # -- generated translated image for class B based on the input generated image for class A\n",
    "        \n",
    "        self.fake_A_ide = self.netG_B2A(real_A)  # G_B2A(A) # -- generated translated image for class A based on the input real image for class A\n",
    "        self.fake_B_ide = self.netG_A2B(real_B)  # G_A2B(B) # -- generated translated image for class A based on the input real image for class A\n",
    "        \n",
    "        return (self.fake_B, self.pred_fake_B, self.rec_A, self.fake_A, self.pred_fake_A, self.rec_B, self.fake_A_ide, self.fake_B_ide) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88f421df-f44e-4925-8c53-83a3c587d281",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CycleGANModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6992318b-e718-4cb8-aea2-c9b9a5e0d3b3",
   "metadata": {},
   "source": [
    "let us feed a sample input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8845c8a-8800-4f6e-8d71-fd9bcd696350",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_A_sample_path = \"annotated_cyclegan/data/horse2zebra/testA/n02381460_1000.jpg\"\n",
    "image_B_sample_path = \"annotated_cyclegan/data/horse2zebra/testB/n02391049_100.jpg\"\n",
    "print(\"input real image for class A\")\n",
    "plt.imshow(plt.imread(image_A_sample_path))\n",
    "plt.show()\n",
    "print(\"input real image for class B\")\n",
    "plt.imshow(plt.imread(image_B_sample_path))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd02563a-6b11-4bf8-93f8-e665aaa9a2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_A = Image.open(image_A_sample_path).convert('RGB')\n",
    "transformed_image_A = get_transform()(image_A)\n",
    "image_B = Image.open(image_B_sample_path).convert('RGB')\n",
    "transformed_image_B = get_transform()(image_B)\n",
    "res = model(transformed_image_A.unsqueeze(0), transformed_image_B.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97c3b42f-9543-42e9-a84e-938f24f541db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor2im(input_image, imtype=np.uint8):\n",
    "    \"\"\"\"Converts a Tensor array into a numpy image array.\n",
    "\n",
    "    Parameters:\n",
    "        input_image (tensor) --  the input image tensor array\n",
    "        imtype (type)        --  the desired type of the converted numpy array\n",
    "    \"\"\"\n",
    "    image_tensor = input_image.data\n",
    "    image_numpy = image_tensor[0].cpu().float().numpy()\n",
    "    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n",
    "    return image_numpy.astype(imtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da04ad58-53a1-4172-a643-c4df998bd1a4",
   "metadata": {},
   "source": [
    "let us see the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c1105a-198e-469e-9fa4-732b158e14c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.fake_B, self.pred_fake_B, self.rec_A, self.fake_A, self.pred_fake_A, self.rec_B, self.fake_A_ide, self.fake_B_ide\n",
    "print(\"generated image for class B based on input real image for class A\")\n",
    "plt.imshow(tensor2im(res[0])) # surely, right now everything is in chaos\n",
    "plt.show()\n",
    "print(\"predict real/fake of generated image for class B based on input real image for class A\")\n",
    "plt.imshow(tensor2im(res[1])) # surely, right now everything is in chaos\n",
    "plt.show()\n",
    "print(\"generated image for class A based on generated image for class B\")\n",
    "plt.imshow(tensor2im(res[2])) # surely, right now everything is in chaos\n",
    "plt.show()\n",
    "print(\"generated image for class A based on input real image for class B\")\n",
    "plt.imshow(tensor2im(res[3])) # surely, right now everything is in chaos\n",
    "plt.show()\n",
    "print(\"predict real/fake of generated image for class A based on input real image for class B\")\n",
    "plt.imshow(tensor2im(res[4])) # surely, right now everything is in chaos\n",
    "plt.show()\n",
    "print(\"generated image for class B based on generated image for class A\")\n",
    "plt.imshow(tensor2im(res[5])) # surely, right now everything is in chaos\n",
    "plt.show()\n",
    "print(\"generated image for class A based on generated image for class A\")\n",
    "plt.imshow(tensor2im(res[6])) # surely, right now everything is in chaos\n",
    "plt.show()\n",
    "print(\"generated image for class B based on generated image for class B\")\n",
    "plt.imshow(tensor2im(res[7])) # surely, right now everything is in chaos\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672647d3-9972-460a-b5b4-4170c9ae5c47",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Design loss function and backward function of the model\n",
    "\n",
    "The overall loss comes from several components: \n",
    "- GAN loss for G_A2B, G_B2A, D_A, D_B; \n",
    "- Cycle consistence loss for real_A vs rec_A and real_B vs rec_B; \n",
    "- Identity loss for real_A vs fake_A_ide and real_B vs fake_B_ide.\n",
    "\n",
    "Here we customized the backward function inside the model based on the loss scheme.\n",
    "> As the paper defines an image pool called `ImagePool` of 50 generated images for each discriminator model that is first populated and probabilistically either adds new images to the pool by replacing an existing image or uses a generated image directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f4aeffe-b44f-4aba-b49a-77a8cbc3fcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePool():\n",
    "    \"\"\"This class implements an image buffer that stores previously generated images.\n",
    "\n",
    "    This buffer enables us to update discriminators using a history of generated images\n",
    "    rather than the ones produced by the latest generators.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pool_size):\n",
    "        \"\"\"Initialize the ImagePool class\n",
    "\n",
    "        Parameters:\n",
    "            pool_size (int) -- the size of image buffer, if pool_size=0, no buffer will be created\n",
    "        \"\"\"\n",
    "        self.pool_size = pool_size\n",
    "        if self.pool_size > 0:  # create an empty pool\n",
    "            self.num_imgs = 0\n",
    "            self.images = []\n",
    "\n",
    "    def query(self, images):\n",
    "        \"\"\"Return an image from the pool.\n",
    "\n",
    "        Parameters:\n",
    "            images: the latest generated images from the generator\n",
    "\n",
    "        Returns images from the buffer.\n",
    "\n",
    "        By 50/100, the buffer will return input images.\n",
    "        By 50/100, the buffer will return images previously stored in the buffer,\n",
    "        and insert the current images to the buffer.\n",
    "        \"\"\"\n",
    "        if self.pool_size == 0:  # if the buffer size is 0, do nothing\n",
    "            return images\n",
    "        return_images = []\n",
    "        for image in images:\n",
    "            image = torch.unsqueeze(image.data, 0)\n",
    "            if self.num_imgs < self.pool_size:   # if the buffer is not full; keep inserting current images to the buffer\n",
    "                self.num_imgs = self.num_imgs + 1\n",
    "                self.images.append(image)\n",
    "                return_images.append(image)\n",
    "            else:\n",
    "                p = random.uniform(0, 1)\n",
    "                if p > 0.5:  # by 50% chance, the buffer will return a previously stored image, and insert the current image into the buffer\n",
    "                    random_id = random.randint(0, self.pool_size - 1)  # randint is inclusive\n",
    "                    tmp = self.images[random_id].clone()\n",
    "                    self.images[random_id] = image\n",
    "                    return_images.append(tmp)\n",
    "                else:       # by another 50% chance, the buffer will return the current image\n",
    "                    return_images.append(image)\n",
    "        return_images = torch.cat(return_images, 0)   # collect all the images and return\n",
    "        return return_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab0a90d8-73e9-4729-a8ce-c777c64ead8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleGANModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CycleGANModel, self).__init__()\n",
    "        self.netG_A2B = G_A2B\n",
    "        self.netG_B2A = G_B2A\n",
    "        self.netD_A = D_A\n",
    "        self.netD_B = D_B\n",
    "        \n",
    "        self.criterionGAN = torch.nn.MSELoss()\n",
    "        self.criterionCycle = torch.nn.L1Loss()\n",
    "        self.criterionIdt = torch.nn.L1Loss()\n",
    "        \n",
    "        self.lambda_idt = 0.5 # 'use identity mapping. Setting lambda_identity other than 0 has an effect of scaling the weight of the identity mapping loss. For example, if the weight of the identity loss should be 10 times smaller than the weight of the reconstruction loss, please set lambda_identity = 0.1'\n",
    "        self.lambda_A = 10.0 # 'weight for cycle loss (B -> A -> B)'\n",
    "        self.lambda_B = 10.0 # 'weight for cycle loss (A -> B -> A)'\n",
    "        \n",
    "        self.lr = 0.0002\n",
    "        self.beta1 = 0.5\n",
    "        self.optimizer_G = torch.optim.Adam(itertools.chain(self.netG_A2B.parameters(), self.netG_B2A.parameters()), lr=self.lr, betas=(self.beta1, 0.999))\n",
    "        self.optimizer_D = torch.optim.Adam(itertools.chain(self.netD_A.parameters(), self.netD_B.parameters()), lr=self.lr, betas=(self.beta1, 0.999))\n",
    "        self.optimizers = [self.optimizer_G, self.optimizer_D]\n",
    "        \n",
    "        self.pool_size = 50\n",
    "        self.fake_A_pool = ImagePool(self.pool_size)  # create image buffer to store previously generated images\n",
    "        self.fake_B_pool = ImagePool(self.pool_size)  # create image buffer to store previously generated images\n",
    "        \n",
    "        self.loss_names = ['D_A', 'G_A2B', 'cycle_A', 'idt_A', 'D_B', 'G_B2A', 'cycle_B', 'idt_B']\n",
    "        \n",
    "    def set_input(self, input):\n",
    "        \"\"\"Unpack input data from the dataloader and perform necessary pre-processing steps.\n",
    "\n",
    "        Parameters:\n",
    "            input (dict): include the data itself and its metadata information.\n",
    "\n",
    "        The option 'direction' can be used to swap domain A and domain B.\n",
    "        \"\"\"\n",
    "        self.real_A = input['A']\n",
    "        self.real_B = input['B']\n",
    "    \n",
    "    def get_current_losses(self):\n",
    "        \"\"\"Return traning losses / errors. train.py will print out these errors on console, and save them to a file\"\"\"\n",
    "        errors_ret = OrderedDict()\n",
    "        for name in self.loss_names:\n",
    "            if isinstance(name, str):\n",
    "                errors_ret[name] = float(getattr(self, 'loss_' + name))  # float(...) works for both scalar tensor and float number\n",
    "        return errors_ret\n",
    "    \n",
    "    def update_learning_rate(self):\n",
    "        \"\"\"Update learning rates for all the networks; called at the end of every epoch\"\"\"\n",
    "        for scheduler in self.schedulers:\n",
    "            scheduler.step()\n",
    "        \n",
    "        \n",
    "    def forward(self): # recall the workflow\n",
    "        self.fake_B = self.netG_A2B(self.real_A)  # G_A2B(A) # -- generated translated image for class B based on the input real image for class A\n",
    "        self.pred_fake_B = self.netD_B(self.fake_B) #D_B(G_A2B(A)) -- predict real/fake of the generated image for class B based on the input real image for class A\n",
    "        self.rec_A = self.netG_B2A(self.fake_B)   # G_A2B(A) # -- generated translated image for class A based on the input generated image for class B\n",
    "\n",
    "        self.fake_A = self.netG_B2A(self.real_B)  # G_A2B(A) # -- generated translated image for class A based on the input real image for class B\n",
    "        self.pred_fake_A = self.netD_A(self.fake_A) #D_A(G_B2A(B)) -- predict real/fake of the generated image for class A based on the input real image for class B\n",
    "        self.rec_B = self.netG_A2B(self.fake_A)   # G_A2B(A) # -- generated translated image for class B based on the input generated image for class A\n",
    "        \n",
    "        self.fake_A_ide = self.netG_B2A(self.real_A)  # G_B2A(A) # -- generated translated image for class A based on the input real image for class A\n",
    "        self.fake_B_ide = self.netG_A2B(self.real_B)  # G_A2B(B) # -- generated translated image for class A based on the input real image for class A\n",
    "        \n",
    "        return (self.fake_B, self.pred_fake_B, self.rec_A, self.fake_A, self.pred_fake_A, self.rec_B, self.fake_A_ide, self.fake_B_ide) \n",
    "    \n",
    "    def backward_D_basic(self, netD, real, fake):\n",
    "        \"\"\"Calculate GAN loss for the discriminator\n",
    "\n",
    "        Parameters:\n",
    "            netD (network)      -- the discriminator D\n",
    "            real (tensor array) -- real images\n",
    "            fake (tensor array) -- images generated by a generator\n",
    "\n",
    "        Return the discriminator loss.\n",
    "        We also call loss_D.backward() to calculate the gradients.\n",
    "        \"\"\"\n",
    "        # Real\n",
    "        pred_real = netD(real)\n",
    "        real_label_tensor = torch.tensor(1.0).expand_as(pred_real)\n",
    "        loss_D_real = self.criterionGAN(pred_real, real_label_tensor)\n",
    "            \n",
    "        # Fake\n",
    "        pred_fake = netD(fake)\n",
    "        fake_label_tensor = torch.tensor(0.0).expand_as(pred_fake)\n",
    "        loss_D_fake = self.criterionGAN(pred_fake, fake_label_tensor)\n",
    "        \n",
    "        # Combined loss and calculate gradients\n",
    "        loss_D = (loss_D_real + loss_D_fake) * 0.5\n",
    "        loss_D.backward()\n",
    "        return loss_D\n",
    "    \n",
    "    \n",
    "    def backward_D_A(self):\n",
    "        \"\"\"Calculate GAN loss for discriminator D_A\"\"\"\n",
    "        fake_A = self.fake_A_pool.query(self.fake_A)\n",
    "        self.loss_D_A = self.backward_D_basic(self.netD_A, self.real_A, fake_A)\n",
    "\n",
    "    def backward_D_B(self):\n",
    "        \"\"\"Calculate GAN loss for discriminator D_B\"\"\"\n",
    "        fake_B = self.fake_B_pool.query(self.fake_B)\n",
    "        self.loss_D_B = self.backward_D_basic(self.netD_B, self.real_B, fake_B)\n",
    "\n",
    "    def backward_G(self):\n",
    "        \"\"\"Calculate the loss for generators G_A and G_B\"\"\"\n",
    "        \n",
    "        # Identity loss\n",
    "        # G_A2B should be identity if real_B is fed: ||G_A2B(B) - B||\n",
    "        self.loss_idt_A = self.criterionIdt(self.fake_A_ide, self.real_A) * self.lambda_A * self.lambda_idt\n",
    "        # G_B2A should be identity if real_A is fed: ||G_B2A(A) - A||\n",
    "        self.loss_idt_B = self.criterionIdt(self.fake_B_ide, self.real_B) * self.lambda_B * self.lambda_idt\n",
    "\n",
    "        # GAN loss D_B(G_A2B(A))        \n",
    "        label_tensor_G_A2B = torch.tensor(1.0).expand_as(self.pred_fake_B) # we want the result of D_B(G_A2B(A)) to be True to train the G, aka, to fool D_B\n",
    "        self.loss_G_A2B = self.criterionGAN(self.pred_fake_B, label_tensor_G_A2B)\n",
    "        \n",
    "        # GAN loss D_A(G_B2A(B)) \n",
    "        label_tensor_G_B2A = torch.tensor(1.0).expand_as(self.pred_fake_A) # we want the result of D_A(G_B2A(B)) to be True to train the G, aka, to fool D_A\n",
    "        self.loss_G_B2A = self.criterionGAN(self.pred_fake_A, label_tensor_G_B2A)\n",
    "                \n",
    "        # cycle loss || G_B2A(G_A2B(A)) - A||\n",
    "        self.loss_cycle_A = self.criterionCycle(self.rec_A, self.real_A) * self.lambda_A\n",
    "        # cycle loss || G_A2B(G_B2A(B)) - B||\n",
    "        self.loss_cycle_B = self.criterionCycle(self.rec_B, self.real_B) * self.lambda_B\n",
    "        \n",
    "        # combined loss and calculate gradients\n",
    "        self.loss_G = self.loss_G_A2B + self.loss_G_B2A + self.loss_cycle_A + self.loss_cycle_B + self.loss_idt_A + self.loss_idt_B\n",
    "        self.loss_G.backward()\n",
    "    \n",
    "    def set_requires_grad(self, nets, requires_grad=False):\n",
    "        \"\"\"Set requies_grad=Fasle for all the networks to avoid unnecessary computations\n",
    "        Parameters:\n",
    "            nets (network list)   -- a list of networks\n",
    "            requires_grad (bool)  -- whether the networks require gradients or not\n",
    "        \"\"\"\n",
    "        if not isinstance(nets, list):\n",
    "            nets = [nets]\n",
    "        for net in nets:\n",
    "            if net is not None:\n",
    "                for param in net.parameters():\n",
    "                    param.requires_grad = requires_grad\n",
    "\n",
    "    def optimize_parameters(self):\n",
    "        \"\"\"Calculate losses, gradients, and update network weights; called in every training iteration\"\"\"\n",
    "        # forward\n",
    "        self.forward()      # compute fake images and reconstruction images.\n",
    "        # G_A2B and G_B2A\n",
    "        self.set_requires_grad([self.netD_A, self.netD_B], False)  # Ds require no gradients when optimizing Gs\n",
    "        self.optimizer_G.zero_grad()  # set G_A and G_B's gradients to zero\n",
    "        self.backward_G()             # calculate gradients for G_A and G_B\n",
    "        self.optimizer_G.step()       # update G_A and G_B's weights\n",
    "        # D_A and D_B\n",
    "        self.set_requires_grad([self.netD_A, self.netD_B], True)\n",
    "        self.optimizer_D.zero_grad()   # set D_A and D_B's gradients to zero\n",
    "        self.backward_D_A()      # calculate gradients for D_A\n",
    "        self.backward_D_B()      # calculate graidents for D_B\n",
    "        self.optimizer_D.step()  # update D_A and D_B's weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f195cd53-9250-4a60-8327-6df5419eb12d",
   "metadata": {},
   "source": [
    "### model training\n",
    "\n",
    "we followed the alternating training approach to update the parameters of generators and discriminator. As mentioned in Google's GAN training [documents](https://developers.google.com/machine-learning/gan/training):\n",
    "\n",
    "```\n",
    "The generator and the discriminator have different training processes. So how do we train the GAN as a whole?\n",
    "\n",
    "GAN training proceeds in alternating periods:\n",
    "\n",
    "The discriminator trains for one or more epochs.\n",
    "The generator trains for one or more epochs.\n",
    "Repeat steps 1 and 2 to continue to train the generator and discriminator networks.\n",
    "We keep the generator constant during the discriminator training phase. As discriminator training tries to figure out how to distinguish real data from fake, it has to learn how to recognize the generator's flaws. That's a different problem for a thoroughly trained generator than it is for an untrained generator that produces random output.\n",
    "\n",
    "Similarly, we keep the discriminator constant during the generator training phase. Otherwise the generator would be trying to hit a moving target and might never converge.\n",
    "\n",
    "It's this back and forth that allows GANs to tackle otherwise intractable generative problems. We get a toehold in the difficult generative problem by starting with a much simpler classification problem. Conversely, if you can't train a classifier to tell the difference between real and generated data even for the initial random generator output, you can't get the GAN training started.\n",
    "\n",
    "Convergence\n",
    "\n",
    "As the generator improves with training, the discriminator performance gets worse because the discriminator can't easily tell the difference between real and fake. If the generator succeeds perfectly, then the discriminator has a 50% accuracy. In effect, the discriminator flips a coin to make its prediction.\n",
    "\n",
    "This progression poses a problem for convergence of the GAN as a whole: the discriminator feedback gets less meaningful over time. If the GAN continues training past the point when the discriminator is giving completely random feedback, then the generator starts to train on junk feedback, and its own quality may collapse.\n",
    "\n",
    "For a GAN, convergence is often a fleeting, rather than stable, state.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4198d33f-3835-4fe0-bc68-981054d9291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(model):\n",
    "    fig, axs = plt.subplots(1,8, figsize = (40, 5))\n",
    "    axs[0].imshow(tensor2im(model.real_A[0:1]))\n",
    "    # plt.show()\n",
    "    axs[1].imshow(tensor2im(model.fake_B[0:1]))\n",
    "    # plt.show()\n",
    "    axs[2].imshow(tensor2im(model.rec_A[0:1]))\n",
    "    # plt.show()\n",
    "    axs[3].imshow(tensor2im(model.fake_A_ide[0:1]))\n",
    "    # plt.show()\n",
    "    axs[4].imshow(tensor2im(model.real_B[0:1]))\n",
    "    # plt.show()\n",
    "    axs[5].imshow(tensor2im(model.fake_A[0:1]))\n",
    "    # plt.show()\n",
    "    axs[6].imshow(tensor2im(model.rec_B[0:1]))\n",
    "    # plt.show()\n",
    "    axs[7].imshow(tensor2im(model.fake_B_ide[0:1]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff615de3-8643-43c8-b1aa-b48ff7d3e9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = 100\n",
    "model = CycleGANModel()\n",
    "for epoch in range(max_epoch):    \n",
    "    print(f\"epoch: {epoch}\")\n",
    "    for i, data in enumerate(train_loader):  # inner loop within one epoch\n",
    "        model.set_input(data)         # unpack data from dataset and apply preprocessing\n",
    "        model.optimize_parameters()   # calculate loss functions, get gradients, update network weights\n",
    "        losses = model.get_current_losses()\n",
    "        print(f\"step: {i}, losses: {losses}\")\n",
    "        print(f\"D_A mean prob: {model.pred_fake_A.mean()}; D_B mean prob: {model.pred_fake_B.mean()}\")\n",
    "        visualize(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a8f89f-65cb-4e5d-8ffb-a2fa2c1cc88a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
