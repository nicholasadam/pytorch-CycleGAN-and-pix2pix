{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9be3d715-de60-475f-809c-1494e4643109",
   "metadata": {},
   "source": [
    "## Annotated CycleGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3a9e01-1da5-4227-a49b-991623e346bc",
   "metadata": {},
   "source": [
    "As we are building a web app \"deepfake\" which is of image translation based on GAN model, I present an \"annotated\" version of the pytorch-based-CycleGAN model in the form of a line-by-line implementation. It will cover data preparation, different GAN models, training schemes, optimization tricks for both training and computation. After completing this notebook, you will know:\n",
    "\n",
    "- How to implement the discriminator and generator models.\n",
    "- How to define composite models to train the two aforementioned models via adversarial and cycle loss.\n",
    "- How to do the model inference to generate the translated image.\n",
    "\n",
    "To follow along you will first need to install the libraries listed in `requirements-an.txt`. The workflow should be considered as a starting point of image translation tasks with the enrichment of GAN model.\n",
    "\n",
    "> My comments are blockquoted. The main text is based on the CycleGAN/Pytorch documentation.\n",
    "> Wangsu Hu (wangsu.hu1234@gmail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e67c89f-4428-4006-8bab-944ef3f6c0a7",
   "metadata": {},
   "source": [
    "## Prelims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cda74abd-2177-466e-a899-a301bfeaea76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/Users/wangsuhu/Google Drive/Github/pytorch-CycleGAN-and-pix2pix/env/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -r requirements-an.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4c38ec-cc00-45e8-802c-711963d09165",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "The Cycle Generative Adversarial Network, or CycleGAN, is an approach to training a deep convolutional neural network for image-to-image translation tasks.\n",
    "\n",
    "Unlike other GAN models for image translation, the CycleGAN does not require a dataset of paired images. \n",
    "\n",
    "The CycleGAN model was described by Jun-Yan Zhu, et al. in their 2017 paper titled [“Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks.”](https://arxiv.org/abs/1703.10593)\n",
    "\n",
    "Here is the official github repo released and maintained by the author list [pytorch-CycleGAN-and-pix2pix](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4993df-d9cd-4c61-be2b-e6f2bd68348a",
   "metadata": {
    "tags": []
   },
   "source": [
    "The model architecture is comprised of two generator models: \n",
    "- one generator (Generator G_A2B) for generating images for the second class (class-B) based on the input image for the first class (class-A)\n",
    "- another generator (Generator G_B2A) for generating images for the first class (class-A) based on the input image for the second class (class-B)\n",
    "\n",
    "> as you can see, unlike normal GAN using randomness, the generator of CycleGAN is conditional on the input image and the input image can be a real image or a generated image. See the generator blocks in workflow below.\n",
    "\n",
    "-- real image for class-A -> G_A2B -> generated image for class-B \n",
    "-- generated image for class-A -> G_A2B -> generated image for class-B \n",
    "-- real image for class-B -> G_B2A -> generated image for class-A \n",
    "-- generated image for class-B -> G_B2A -> generated image for class-A \n",
    "\n",
    "- one discriminator (Discriminator D_A) takes images from class-A then predicts whether they are real or fake.\n",
    "- one discriminator (Discriminator D_B) takes images from class-B then predicts whether they are real or fake.\n",
    "\n",
    "> similar to generators, the input image of discriminator can be a real image or a generated image.\n",
    "\n",
    "-- real image for class-A -> D_A -> real/fake\n",
    "-- generated image for class-A -> D_A -> real/fake\n",
    "-- real image for class-B -> D_B -> real/fake\n",
    "-- generated image for class-B -> D_B -> real/fake\n",
    "\n",
    "\n",
    "Why does sucg GAN model called \"Cycle\"? \n",
    "\n",
    "The discriminator and generator models are trained in an adversarial zero-sum process, like normal GAN models. The generators learn to better fool the discriminators and the discriminator learn to better detect fake images. Together, the models find an equilibrium during the training process.\n",
    "\n",
    "Additionally, the generator models are regularized to not just create new images in the target domain, but instead translate more reconstructed versions of the input images from the source domain. This is achieved by using generated images as input to the corresponding generator model and comparing the output image to the original images. Passing an image through both generators is called a cycle. Together, each pair of generator models are trained to better reproduce the original source image, referred to as cycle consistency.\n",
    "\n",
    "-- real image for class-A -> G_A2B -> generated image for class-B -> G_B2A -> generated image for class-A vs the input real image for class-A\n",
    "-- real image for class-B -> G_B2A -> generated image for class-A -> G_A2B -> generated image for class-B vs the input real image for class-B\n",
    "\n",
    "see the following flow chart for your reference.\n",
    "![work_flow](workflow.png)\n",
    "\n",
    "\n",
    "> we dont consider the \"identity mapping\" architecture here for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721eeb7e-118a-4a92-b58e-4a9757543e85",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49c5094-dce2-47a5-b5ed-970037ae802f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
